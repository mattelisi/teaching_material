---
title: 'Overdispersed poisson GLMM example: police stops in NYC'
author: "Matteo Lisi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)

library(tidyverse)
library(rstan)
library(tidybayes)
```

## Introduction

In this document, we analyze an example dataset of police stop-and-frisk data, referred to in `police_BDA3rd_p435.pdf`.  
The data contains counts of stops by precinct, ethnicity, and crime type, along with population and DCJS arrests.  
We’ll fit an **overdispersed Poisson** regression with random intercepts by precinct and an observation-level random effect, following Gelman & Hill’s approach.

## 1. Load and Explore the Data

```{r load-data}
# Read the data from a text file
d <- read_delim("police_stops.txt", delim = " ", col_names = TRUE)

# Examine the structure
str(d)

# Note:
# precincts are numbered 1-75
# ethnicity 1=black, 2=hispanic, 3=white
# crime type 1=violent, 2=weapons, 3=property, 4=drug
```

---

## Summary Statistics

We’ll group by ethnicity to see how many stops, how many arrests, and what fraction of population each group represents across all precincts/crime categories.

```{r summary-stats}
d_summaries <- d %>%
  mutate(eth = case_when(
    eth == 1 ~ "black",
    eth == 2 ~ "hispanic",
    eth == 3 ~ "white"
  )) %>%
  group_by(eth) %>%
  summarise(
    pop           = sum(pop),
    total_stops   = sum(stops),
    total_arrests = sum(past.arrests),
    .groups = "drop"
  ) %>%
  mutate(
    prop_of_all_stops = total_stops / sum(total_stops),
    pop_fraction      = pop / sum(pop)
  )

d_summaries
```

---

## GLM for count data

The type of data (frequency of police stops) is referred to as "count" data, and it is unsuitable for analysis with a linear regression approach (for example because it is discrete, composed of integers only, and usually has a large positive skew and a systematice relation betweeen mean and variance that violates homoscedasticity assumption). There are specific GLM models designed for this type  of data, and one example is the Poisson GLM.

A Poisson model is usually formulated as follow: let $y=(y_1, \dots, y_n)$ be the dependent variable, consisting of counts (non-negative integers) and $x$ the independent variable (predictor). Then

$$
y_i \sim \text{Poisson} \left(\lambda_i \right)
$$
where^[This is the most common formulation, and is sometime referred to as log 'link' function, to indicate the fact that we have a function (the logarithm function) that 'link' the linear part of the model (the linear combination of the independent variables, here $\beta_0 + \beta_1 x_i$) with the parameter of the distribution of the dependent variable (here the Poisson rate parameter $\lambda$). This is a common component to all generalized linear models, for example for the logistic regression we have a 'logit' link function - the quantile function of the standard logistic distribution - that link the linear predictor part with the parameter $p$ of the binomial distribution.]
$$
\log( \lambda_i) = \beta_0 + \beta_1 x_i
$$
or alternatively 
$$
\lambda_i = e^{\beta_0 + \beta_1 x_i}
$$
The log link is central to Poisson regression. It ensures that predictions for $\lambda_i$ stay strictly positive and also encodes the assumption that changes in $x$ have a multiplicative effect on the rate $\lambda_i$. In practice, that means a one-unit increase in $x$ multiplies the rate by $e^{\beta_1}$ 

The above also show how we can use the exponential function (in R the function `exp()`) to calculate the predicted values of the mean (more precisely, the expected value, $\mathbb{E}(y)$) and variance ($Var(y)$) of the dependent variable from the model's beta coefficients. This relies on the property of the Poisson distribution^[The symbol $\implies$ is used to denote logical implication, e.g. $(A=B)\implies (B=A)$ - symmetry of logical equivalence.]
$$
y_i \sim \text{Poisson} \left(\lambda \right) \implies \mathbb{E}(y)=Var(y)=\lambda
$$

In a Poisson GLM, the model automatically ties the variance to the mean, so if $\lambda_i$ is large, the variance is also large. This is usually an improvement over ordinary least squares (which assumes constant variance), but in real applications we often observe even more variability than a Poisson would allow—so-called **overdispersion**.

In our police-stops example, we suspect overdispersion because the number of stops varies more widely across precincts and time periods than a basic Poisson might allow. We therefore include a random intercept by precinct and an observation-level random term to handle extra variation. This approach is often called an overdispersed Poisson or “Poisson-lognormal” model, depending on the exact formulation.

### How does over-dispersion look like?

The type of data expected under a Poisson model is illustrated in the figure below, which shows 100 datapoints simulated from the model $y_i \sim \text{Poisson} \left(e^{1 + 2x_i }\right)$. The vertical deviations of the datapoints from the line are consistent with the property of the Poisson distribution that the variance of hte count has the same value as their expected value, formally, $Var(y) = \mathbb{E}(y)$).

```{r, fig.width=4, fig.height=4.2, fig.align='center'}
set.seed(2)
n <- 100
x <- runif(n, -1.5, 1.5)
a <- 1
b <- 2
linpred <- a + b*x # linear predictor part
y <- rpois(n, exp(linpred)) # simulate Poisson observations

plot(x,y,col="blue") # plot
x_ <- seq(-2,2,0.1)
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y, lwd=1,lty=3, col="red")


```

We can adjust the code above to simulate the same data with some degree of over-dispersion, by adding additional normally distributed noise ($\epsilon_i \sim \mathcal{N}(0, \sigma_{\epsilon})$) at the observation level: $y_i \sim \text{Poisson} \left(e^{1 + 2x_i +\epsilon_i }\right)$ Varying the variability of this additional noise ($\sigma_{\epsilon}$) regulates the degree of overdispersion. Importantly, these datapoints are simulated assuming the same function for the average (expected) number of counts (same also as the previous figure), they just differ in the amount of overdispersion relative to a Poisson model. 


```{r, fig.width=6, fig.height=3.5, fig.align='center', echo=FALSE}
set.seed(2)
n <- 100
x <- runif(n, -1.5, 1.5)
a <- 1
b <- 2
linpred <- a + b*x # linear predictor part

y0 <- rpois(n, exp(linpred + rnorm(n, mean=0, sd=0.5))) # simulate negative binomial
y1 <- rpois(n, exp(linpred + rnorm(n, mean=0, sd=1))) # simulate negative binomial



par(mfrow=c(1,2))
x_ <- seq(-2,2,0.1)

plot(x,y0,col="blue", ylim=c(0,125), main=expression(sigma[epsilon]==0.5),ylab="y") # plot
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y0, lwd=1,lty=3, col="red")


plot(x,y1,col="blue", ylim=c(0,125), main=expression(sigma[epsilon]==1),ylab="y") # plot
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y1, lwd=1,lty=3, col="red")

```

---


## Model Definition

We define the overdispersed Poisson model:

\[
y_{ep} \;\sim\; \text{Poisson}\bigl(\underbrace{\text{past.arrests}_{ep} \times \tfrac{15}{12}}_{\text{offset term}} \;\times\; e^{\,\alpha_{e} + \beta_{p} + \epsilon_{ep}} \bigr),
\]

where:

- \( y_{ep} \) = number of stops for ethnicity \( e \) in precinct \( p \).  
- \( \alpha_{e} \) = fixed effect for each ethnicity (\( e = 1,2,3\)).  
- \( \beta_{p} \) = random effect for precinct \( p \), with \( \beta_p \sim N(0, \sigma_\beta^2). \)  
- \( \epsilon_{ep} \) = observation-level overdispersion, \( \epsilon_{ep} \sim N(0, \sigma_\epsilon^2). \)  

Hence, the log-rate is: \(\log(\lambda_{ep}) = \alpha_e + \beta_p + \epsilon_{ep}\), and we multiply \(\lambda_{ep}\) by the offset \(\text{past.arrests}_{ep} \times \tfrac{15}{12}\).

### Non-Centered Parametrization

We call \(\beta_p\) *non-centered* if we write:

\[
\beta_p = \sigma_\beta \;\beta_{\text{raw},p}, \quad \beta_{\text{raw},p} \sim N(0,1),
\]

instead of sampling \(\beta_p\) directly from \(N(0,\sigma_\beta^2)\). This often improves sampling efficiency in hierarchical models. The same logic applies to \(\epsilon_{ep}\).

---

## Stan Code

Below is the Stan code (which you can save in a file named `overdispersed_poisson.stan`):

```stan
data {
  int<lower=1> N;          // total # of (eth, precinct) data points
  int<lower=1> n_eth;      // number of ethnicity categories, e.g. 3
  int<lower=1> n_prec;     // number of precincts
  
  int<lower=0> y[N];       // outcome counts y_{ep}
  vector[N] past_arrests;  // baseline/reference rate
  
  int<lower=1, upper=n_eth> eth[N];      // ethnicity ID for each row
  int<lower=1, upper=n_prec> precinct[N]; // precinct ID for each row
}

parameters {
  // Ethnicity effects (fixed)
  vector[n_eth] alpha; 
  
  // Random intercepts for precinct
  real<lower=0> sigma_beta; 
  vector[n_prec] beta_raw;   // non-centered param for precinct

  // Overdispersion
  real<lower=0> sigma_eps;
  vector[N] eps_raw;         // non-centered param for each (e,p) observation
}

transformed parameters {
  vector[n_prec] beta; 
  beta = sigma_beta * beta_raw;

  vector[N] eps;
  eps = sigma_eps * eps_raw;
}

model {
  // Priors (adjust as appropriate)
  alpha ~ normal(0, 5);           
  sigma_beta ~ exponential(1);    
  beta_raw ~ normal(0, 1);

  sigma_eps ~ exponential(1);     
  eps_raw ~ normal(0, 1);
  
  // Likelihood
  for (i in 1:N) {
    // lambda = alpha[ eth[i] ] + beta[ precinct[i] ] + eps[i]
    // Multiply by (past_arrests[i] * 15/12) to get Poisson mean
    y[i] ~ poisson(past_arrests[i] * (15.0 / 12.0) * exp(alpha[eth[i]] + beta[precinct[i]] + eps[i]));
  }
}
```

---

## Prior Predictive Check

Before fitting the model, it’s good practice to simulate data under the prior to see if the implied distribution is reasonable. For brevity, here’s a minimal example:

```{r prior-predictive}
# Suppose we do a quick simulation from the prior in R:
# (We won't run Stan's generated quantities block, but just do a rough check.)

set.seed(123)
N_sim <- 10
sigma_beta_sim <- rexp(1, 1)
sigma_eps_sim <- rexp(1, 1)
alpha_sim <- rnorm(3, 0, 5) # 3 ethnicity groups

beta_raw_sim <- rnorm(75, 0, 1)
beta_sim <- sigma_beta_sim * beta_raw_sim

eps_raw_sim <- rnorm(N_sim, 0, 1)
eps_sim <- sigma_eps_sim * eps_raw_sim

cat("Simulated sigma_beta =", sigma_beta_sim, "\n")
cat("Simulated sigma_eps  =", sigma_eps_sim, "\n")
cat("Simulated alpha =", alpha_sim, "\n")
```

We could draw some random `past_arrests` values (e.g., 1–50) and see how big the Poisson means might get. If these are unreasonably large or small, we might tighten the priors.

---

## Fit the Model

**Important**: If `past.arrests == 0` for some rows but `stops` are nonzero, we need to avoid a zero Poisson mean. One quick fix is to add a small constant (e.g. 0.5) to `past.arrests`.

```{r correct-past-arrests}
# Correction for zero arrests
d$past.arrests <- ifelse(d$past.arrests == 0, 0.5, d$past.arrests)
```

### Prepare data for Stan

```{r prepare-stan-data}
stan_data <- list(
  N         = nrow(d),
  n_eth     = length(unique(d$eth)),
  n_prec    = length(unique(d$precinct)),
  y         = d$stops,
  eth       = d$eth,
  past_arrests  = d$past.arrests,
  precinct  = d$precinct
)
```

### Compile and Run Stan

```{r stan-fit, results='markup', eval=FALSE}
# Make sure you have the Stan model saved as "overdispersed_poisson.stan".
# Then run:
fit <- stan(
  file = "overdispersed_poisson.stan",
  data = stan_data,
  iter = 2000,
  chains = 4,
  cores = 4
)

```

```{r include=FALSE}
fit <- readRDS("fit_01.RDS")
```


```{r stan-output-print, results='markup'}
print(fit, pars = c("alpha", "sigma_beta", "sigma_eps"))
```


Here, `alpha` are the ethnic-group log-effects, `sigma_beta` is the precinct-level standard deviation, and `sigma_eps` is the overdispersion. You can also examine random intercepts `beta` or the `eps` in detail.

## Diagnostic Checks

```{r diagnostics}
# Traceplots
traceplot(fit, pars = c("alpha", "sigma_beta", "sigma_eps"))

# Pairs plot for some key parameters
pairs(fit, pars = c("alpha[1]", "alpha[2]", "alpha[3]", "sigma_beta", "sigma_eps"))
```

Check for good mixing (no major issues in traceplots) and no suspicious funnel shapes in pairs plots.

---

## Posterior Analysis

We use **tidybayes** to extract samples and summarize. Suppose we want to look at the exponentiated ethnic effects (relative stops vs. arrests).

```{r posterior-analysis}
library(tidybayes)

posterior <- fit %>% 
  spread_draws(alpha[e]) %>%
  mutate(rate = exp(alpha))  # exponentiate to interpret as a multiplicative factor

# Summarize the rate by ethnicity
posterior %>%
  group_by(e) %>%
  mean_hdi(rate, .width = 0.95)
```

If `e=1` = black, `e=2` = hispanic, `e=3` = white, these give the average ratio of stops to arrests (relative to some baseline) for each ethnicity, controlling for precinct and overdispersion.



You might then compare black vs. white, etc., by calculating differences or ratios in the posterior draws.

```{r compare-eth}
posterior_ratios <- posterior %>%
  pivot_wider(
    id_cols = c(.chain, .iteration, .draw),  # these 3 columns identify each draw
    names_from = e,                          # which column to pivot on
    values_from = rate,                      # which column holds the values
    names_prefix = "eth_"
  ) %>%
  mutate(
    black_vs_white = eth_1 / eth_3,
    hisp_vs_white  = eth_2 / eth_3
  ) %>%
   # Pivot longer so each ratio is in its own row
  pivot_longer(
    cols = c(black_vs_white, hisp_vs_white),
    names_to = "contrast",
    values_to = "ratio"
  ) %>%
  # Summarize with mean_hdi -> returns columns .mean, .lower, .upper
  group_by(contrast) %>%
  mean_hdi(ratio, .width = 0.95)

posterior_ratios
```

This gives an estimate of how many times more likely black (or hispanic) stops are relative to white stops, after controlling for arrests, precinct differences, and overdispersion.

We can use this for plotting e.g.:

```{r fig.align='center', fig.width=2.5, fig.height=3}
ggplot(posterior_ratios, aes(x = contrast, y = ratio)) +
  geom_point(size=3) +
  geom_hline(yintercept=1, lty=2)+
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.1) +
  labs(
    x = NULL,
    y = "Ratio relative to White",
    title = "Ethnicity vs. White Stop Ratios",
    caption = "Points = posterior means; bars = 95% HDI"
  )
```

---

## Prior predictive check

```{r fig.align='center', fig.width=5, fig.height=3}
N <- nrow(d)
n_eth <- length(unique(d$eth))
n_prec <- length(unique(d$precinct))

# Decide how many draws from the prior to simulate
n_sims <- 200  

# We'll store all simulated Y values in a single big vector
# so we can compare them to the observed distribution
all_prior_stops <- numeric(0)

set.seed(101)  # reproducibility

for (s in 1:n_sims) {
  
  # 1) Sample the parameters from their priors
  
   
  alpha_draw <- rnorm(n_eth, mean = 0, sd = 5) # alpha: one for each ethnicity
  sigma_beta_draw <- rexp(1, rate = 1)
  beta_raw_draw <- rnorm(n_prec, 0, 1)
  beta_draw <- sigma_beta_draw * beta_raw_draw
  
  sigma_eps_draw <- rexp(1, rate = 1)          # Overdispersion
  eps_raw_draw <- rnorm(N, 0, 1)
  eps_draw <- sigma_eps_draw * eps_raw_draw
  
  # 2) Generate y (stops) from Poisson for each observation i

  past_arrests_offset <- ifelse(d$past.arrests == 0, 0.5, d$past.arrests)
  
  # compute the linear predictor for each row i
  lambda_vec <- numeric(N)
  for (i in seq_len(N)) {
    e <- d$eth[i]
    p <- d$precinct[i]
    lambda_vec[i] <- alpha_draw[e] + beta_draw[p] + eps_draw[i]
  }
  
  # Poisson means
  mu_vec <- past_arrests_offset * (15/12) * exp(lambda_vec)
  
  # sample from Poisson
  y_sim <- rpois(N, mu_vec)
  
  # 3) Store in a big vector for later comparison
  all_prior_stops <- c(all_prior_stops, y_sim)
}

# Now we have n_sims * N simulated 'stops' from the prior
# Compare with the empirical distribution:
observed_stops <- d$stops

df_plot <- tibble(
  value = c((observed_stops ), (all_prior_stops )),
  type  = rep(c("Empirical", "Prior"), c(length(observed_stops), length(all_prior_stops)))
)

# Plot densities
ggplot(df_plot, aes(x = value, fill = type)) +
  geom_density(alpha = 0.4) +
  labs(
    x = "stops",
    y = "Density",
    title = "Prior Predictive vs. Empirical Distribution (log scale)",
    fill = "Data Source"
  ) +
  theme_minimal()+
  scale_x_log10()
```



## References

- Gelman, Andrew, and Jennifer Hill. *Data Analysis Using Regression and Multilevel/Hierarchical Models*. 2007.  

